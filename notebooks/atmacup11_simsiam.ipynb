{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "atmacup11_3",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc4kT9qJRCka",
        "outputId": "9f4d13b9-34e8-4b37-9f32-6b8e4adbf374"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-HJd0QWTRMR-",
        "outputId": "1dfd7d1a-a3df-48e1-8d57-25be40392088"
      },
      "source": [
        "import os\n",
        "path = '/content/drive/My Drive/kaggle/atmacup11/'\n",
        "os.chdir(path) # カレントディレクトリを指定\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/kaggle/atmacup11'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0tUuAp8tRTBb",
        "outputId": "5e9953ea-25d0-4632-a10f-e67b5441be1c"
      },
      "source": [
        "!pip install lightly\n",
        "!pip uninstall -y scikit-learn\n",
        "!pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn\n",
        "!pip install ttach\n",
        "!pip install optuna"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lightly\n",
            "  Downloading lightly-1.1.15-py3-none-any.whl (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 174 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 184 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 204 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 215 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 240 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from lightly) (0.10.0+cu102)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from lightly) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from lightly) (2.8.1)\n",
            "Collecting pytorch-lightning>=1.0.4\n",
            "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
            "\u001b[K     |████████████████████████████████| 813 kB 13.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/dist-packages (from lightly) (57.2.0)\n",
            "Collecting hydra-core>=1.0.0\n",
            "  Downloading hydra_core-1.1.0-py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 23.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.15.0)\n",
            "Collecting lightly-utils==0.0.1\n",
            "  Downloading lightly_utils-0.0.1-py3-none-any.whl (6.3 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from lightly) (2021.5.30)\n",
            "Collecting tqdm>=4.44\n",
            "  Downloading tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from lightly) (1.24.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from lightly-utils==0.0.1->lightly) (7.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.0.0->lightly) (5.2.0)\n",
            "Collecting omegaconf==2.1.*\n",
            "  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 29.3 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 29.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (21.0)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 36.3 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.4->lightly) (1.9.0+cu102)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 36.4 MB/s \n",
            "\u001b[?25hCollecting tensorboard!=2.5.0,>=2.2.0\n",
            "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 55.6 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning>=1.0.4->lightly) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->lightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->lightly) (2.10)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.36.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.34.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.32.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.17.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning>=1.0.4->lightly) (3.7.4.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.5.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, future\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=b2bbd20d875cdc8b3442af82da24aef82ecde8cff6b9b44975f9b94504420307\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=8a76b6f94260cab0c8382cc92707d027cdd4c1739d051e92f5972383ecf81819\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built antlr4-python3-runtime future\n",
            "Installing collected packages: multidict, yarl, async-timeout, PyYAML, fsspec, antlr4-python3-runtime, aiohttp, tqdm, torchmetrics, tensorboard, pyDeprecate, omegaconf, future, pytorch-lightning, lightly-utils, hydra-core, lightly\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 antlr4-python3-runtime-4.8 async-timeout-3.0.1 fsspec-2021.7.0 future-0.18.2 hydra-core-1.1.0 lightly-1.1.15 lightly-utils-0.0.1 multidict-5.1.0 omegaconf-2.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 tensorboard-2.4.1 torchmetrics-0.4.1 tqdm-4.61.2 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: scikit-learn 0.22.2.post1\n",
            "Uninstalling scikit-learn-0.22.2.post1:\n",
            "  Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.anaconda.org/scipy-wheels-nightly/simple\n",
            "Collecting scikit-learn\n",
            "  Downloading https://pypi.anaconda.org/scipy-wheels-nightly/simple/scikit-learn/1.0.dev0/scikit_learn-1.0.dev0-cp37-cp37m-manylinux2010_x86_64.whl (22.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.9 MB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "Successfully installed scikit-learn-1.0.dev0 threadpoolctl-2.2.0\n",
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.8.0-py3-none-any.whl (301 kB)\n",
            "\u001b[K     |████████████████████████████████| 301 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.61.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.20)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.6.5-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 14.9 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.8.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.6.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 13.2 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.1.2-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 13.5 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.4.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->alembic->optuna) (1.15.0)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=d9a7f8d1cb09c7735c8abf7102cebe1e3818d4d71468076a5bdc8b70ff0c3a65\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, colorama, stevedore, python-editor, Mako, cmd2, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.4 alembic-1.6.5 cliff-3.8.0 cmaes-0.8.2 cmd2-2.1.2 colorama-0.4.4 colorlog-5.0.1 optuna-2.8.0 pbr-5.6.0 pyperclip-1.8.2 python-editor-1.0.4 stevedore-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8hPjIyERY8S"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB9dvUfpRXDv"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import lightly"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufOk7eb5R-jJ"
      },
      "source": [
        "num_workers = 2\n",
        "batch_size = 256\n",
        "seed = 1\n",
        "epochs = 400\n",
        "input_size = 224\n",
        "\n",
        "# dimension of the embeddings\n",
        "num_ftrs = 512\n",
        "# dimension of the output of the prediction and projection heads\n",
        "out_dim = proj_hidden_dim = 512\n",
        "# the prediction head uses a bottleneck architecture\n",
        "pred_hidden_dim = 128\n",
        "# use 2 layers in the projection head\n",
        "num_mlp_layers = 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN3t9rOiSDXA"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# set the path to the dataset\n",
        "dataset_root = os.path.join(path, 'dataset_atmaCup11/')\n",
        "assert dataset_root is not None\n",
        "\n",
        "input_dir = os.path.join(dataset_root)\n",
        "path_to_data = os.path.join(input_dir, \"photos\")\n",
        "# path_to_data = './imgs/'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnpAkbZ4SNEy"
      },
      "source": [
        "# Data Augmentation\n",
        "collate_fn = lightly.data.ImageCollateFunction(\n",
        "    input_size=input_size,\n",
        "    # require invariance to flips and rotations\n",
        "    hf_prob=0.5,\n",
        "    vf_prob=0.5,\n",
        "    rr_prob=0.5,\n",
        "    # satellite images are all taken from the same height\n",
        "    # so we use only slight random cropping\n",
        "    min_scale=0.5,\n",
        "    # use a weak color jitter for invariance w.r.t small color changes\n",
        "    cj_prob=0.2,\n",
        "    cj_bright=0.1,\n",
        "    cj_contrast=0.1,\n",
        "    cj_hue=0.1,\n",
        "    cj_sat=0.1,\n",
        ")\n",
        "\n",
        "dataset_train_simsiam = lightly.data.LightlyDataset(\n",
        "    input_dir=path_to_data\n",
        ")\n",
        "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
        "    dataset_train_simsiam,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((input_size, input_size)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=lightly.data.collate.imagenet_normalize['mean'],\n",
        "        std=lightly.data.collate.imagenet_normalize['std'],\n",
        "    )\n",
        "])\n",
        "\n",
        "dataset_test = lightly.data.LightlyDataset(\n",
        "    input_dir=path_to_data,\n",
        "    transform=test_transforms\n",
        ")\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es4v3IlrSpXk"
      },
      "source": [
        "resnet = torchvision.models.resnet18(pretrained=False)\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "# create the SimSiam model using the backbone from above\n",
        "model = lightly.models.SimSiam(\n",
        "    backbone,\n",
        "    num_ftrs=num_ftrs,\n",
        "    proj_hidden_dim=pred_hidden_dim,\n",
        "    pred_hidden_dim=pred_hidden_dim,\n",
        "    out_dim=out_dim,\n",
        "    num_mlp_layers=num_mlp_layers\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2BooT1OTVPB"
      },
      "source": [
        "# SimSiam uses a symmetric negative cosine similarity loss\n",
        "criterion = lightly.loss.SymNegCosineSimilarityLoss()\n",
        "\n",
        "# scale the learning rate\n",
        "lr = 0.05 * batch_size / 256\n",
        "# use SGD with momentum and weight decay\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=lr,\n",
        "    weight_decay=5e-4\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPt8Fz-aTccm",
        "outputId": "23ec50ab-9180-490f-9f26-437aca3bc22c"
      },
      "source": [
        "%%time\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "avg_loss = 0.\n",
        "avg_output_std = 0.\n",
        "for e in range(epochs):\n",
        "\n",
        "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
        "\n",
        "        # move images to the gpu\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "\n",
        "        # run the model on both transforms of the images\n",
        "        # the output of the simsiam model is a y containing the predictions\n",
        "        # and projections for each input x\n",
        "        y0, y1 = model(x0, x1)\n",
        "\n",
        "        # backpropagation\n",
        "        loss = criterion(y0, y1)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate the per-dimension standard deviation of the outputs\n",
        "        # we can use this later to check whether the embeddings are collapsing\n",
        "        output, _ = y0\n",
        "        output = output.detach()\n",
        "        output = torch.nn.functional.normalize(output, dim=1)\n",
        "\n",
        "        output_std = torch.std(output, 0)\n",
        "        output_std = output_std.mean()\n",
        "\n",
        "        # use moving averages to track the loss and standard deviation\n",
        "        w = 0.9\n",
        "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
        "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
        "\n",
        "    # the level of collapse is large if the standard deviation of the l2\n",
        "    # normalized output is much smaller than 1 / sqrt(dim)\n",
        "    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n",
        "    # print intermediate results\n",
        "    print(f'[Epoch {e:3d}] '\n",
        "        f'Loss = {avg_loss:.2f} | '\n",
        "        f'Collapse Level: {collapse_level:.2f} / 1.00')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch   0] Loss = -0.59 | Collapse Level: 0.28 / 1.00\n",
            "[Epoch   1] Loss = -0.69 | Collapse Level: 0.31 / 1.00\n",
            "[Epoch   2] Loss = -0.73 | Collapse Level: 0.39 / 1.00\n",
            "[Epoch   3] Loss = -0.73 | Collapse Level: 0.42 / 1.00\n",
            "[Epoch   4] Loss = -0.68 | Collapse Level: 0.49 / 1.00\n",
            "[Epoch   5] Loss = -0.74 | Collapse Level: 0.45 / 1.00\n",
            "[Epoch   6] Loss = -0.77 | Collapse Level: 0.56 / 1.00\n",
            "[Epoch   7] Loss = -0.76 | Collapse Level: 0.49 / 1.00\n",
            "[Epoch   8] Loss = -0.73 | Collapse Level: 0.54 / 1.00\n",
            "[Epoch   9] Loss = -0.69 | Collapse Level: 0.45 / 1.00\n",
            "[Epoch  10] Loss = -0.69 | Collapse Level: 0.47 / 1.00\n",
            "[Epoch  11] Loss = -0.81 | Collapse Level: 0.53 / 1.00\n",
            "[Epoch  12] Loss = -0.73 | Collapse Level: 0.52 / 1.00\n",
            "[Epoch  13] Loss = -0.84 | Collapse Level: 0.63 / 1.00\n",
            "[Epoch  14] Loss = -0.81 | Collapse Level: 0.61 / 1.00\n",
            "[Epoch  15] Loss = -0.71 | Collapse Level: 0.51 / 1.00\n",
            "[Epoch  16] Loss = -0.83 | Collapse Level: 0.67 / 1.00\n",
            "[Epoch  17] Loss = -0.74 | Collapse Level: 0.57 / 1.00\n",
            "[Epoch  18] Loss = -0.78 | Collapse Level: 0.56 / 1.00\n",
            "[Epoch  19] Loss = -0.75 | Collapse Level: 0.62 / 1.00\n",
            "[Epoch  20] Loss = -0.85 | Collapse Level: 0.69 / 1.00\n",
            "[Epoch  21] Loss = -0.77 | Collapse Level: 0.68 / 1.00\n",
            "[Epoch  22] Loss = -0.71 | Collapse Level: 0.59 / 1.00\n",
            "[Epoch  23] Loss = -0.77 | Collapse Level: 0.66 / 1.00\n",
            "[Epoch  24] Loss = -0.84 | Collapse Level: 0.67 / 1.00\n",
            "[Epoch  25] Loss = -0.72 | Collapse Level: 0.63 / 1.00\n",
            "[Epoch  26] Loss = -0.77 | Collapse Level: 0.65 / 1.00\n",
            "[Epoch  27] Loss = -0.79 | Collapse Level: 0.67 / 1.00\n",
            "[Epoch  28] Loss = -0.74 | Collapse Level: 0.39 / 1.00\n",
            "[Epoch  29] Loss = -0.78 | Collapse Level: 0.42 / 1.00\n",
            "[Epoch  30] Loss = -0.80 | Collapse Level: 0.49 / 1.00\n",
            "[Epoch  31] Loss = -0.82 | Collapse Level: 0.56 / 1.00\n",
            "[Epoch  32] Loss = -0.85 | Collapse Level: 0.60 / 1.00\n",
            "[Epoch  33] Loss = -0.82 | Collapse Level: 0.63 / 1.00\n",
            "[Epoch  34] Loss = -0.84 | Collapse Level: 0.61 / 1.00\n",
            "[Epoch  35] Loss = -0.76 | Collapse Level: 0.60 / 1.00\n",
            "[Epoch  36] Loss = -0.83 | Collapse Level: 0.65 / 1.00\n",
            "[Epoch  37] Loss = -0.87 | Collapse Level: 0.68 / 1.00\n",
            "[Epoch  38] Loss = -0.86 | Collapse Level: 0.68 / 1.00\n",
            "[Epoch  39] Loss = -0.91 | Collapse Level: 0.76 / 1.00\n",
            "[Epoch  40] Loss = -0.83 | Collapse Level: 0.75 / 1.00\n",
            "[Epoch  41] Loss = -0.80 | Collapse Level: 0.74 / 1.00\n",
            "[Epoch  42] Loss = -0.81 | Collapse Level: 0.77 / 1.00\n",
            "[Epoch  43] Loss = -0.84 | Collapse Level: 0.78 / 1.00\n",
            "[Epoch  44] Loss = -0.79 | Collapse Level: 0.79 / 1.00\n",
            "[Epoch  45] Loss = -0.81 | Collapse Level: 0.75 / 1.00\n",
            "[Epoch  46] Loss = -0.89 | Collapse Level: 0.79 / 1.00\n",
            "[Epoch  47] Loss = -0.86 | Collapse Level: 0.84 / 1.00\n",
            "[Epoch  48] Loss = -0.86 | Collapse Level: 0.82 / 1.00\n",
            "[Epoch  49] Loss = -0.86 | Collapse Level: 0.79 / 1.00\n",
            "[Epoch  50] Loss = -0.84 | Collapse Level: 0.82 / 1.00\n",
            "[Epoch  51] Loss = -0.75 | Collapse Level: 0.70 / 1.00\n",
            "[Epoch  52] Loss = -0.82 | Collapse Level: 0.70 / 1.00\n",
            "[Epoch  53] Loss = -0.74 | Collapse Level: 0.71 / 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDnn59LUT2QF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84216f6a-bae8-4e79-c857-2408b5e28070"
      },
      "source": [
        "backbone = model.backbone\n",
        "# backbone.add_module('fc', nn.Linear(in_features=512, out_features=1, bias=True))\n",
        "backbone"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erny_sLwWepX"
      },
      "source": [
        "output_dir = os.path.join(path, 'output/')\n",
        "torch.save(backbone.state_dict(), os.path.join(output_dir, './simsiam/model.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_7l37SxpqyN"
      },
      "source": [
        "# Train & Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wwDx9Hh3GAL"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import  glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib_venn import venn2\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "\n",
        "import torch\n",
        "from torchvision.models import resnet18, resnet34, resnet50\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils import data\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedGroupKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUDK8cmVl6Em"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "input_dir = os.path.join(path, 'dataset_atmaCup11/')\n",
        "photo_dir = os.path.join(input_dir, \"photos\")\n",
        "\n",
        "output_dir = os.path.join(path, 'output/')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "photo_pathes = glob(os.path.join(photo_dir, \"*.jpg\"))\n",
        "\n",
        "def seed_torch(seed=510):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6HmrWyPp8Zz"
      },
      "source": [
        "EPOCHS = 200\n",
        "IS_REGR = False\n",
        "IS_THRESHOLD = False\n",
        "IS_SOFTLABEL = False\n",
        "LERNING_RATE = 1e-3\n",
        "LERNING_RATE = 1e-4\n",
        "BATCH_SIZE = 256\n",
        "SIZE = (224, 224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXZ5Dix7qCI5"
      },
      "source": [
        "train_df = pd.read_csv(os.path.join(input_dir, 'train.csv'))\n",
        "test_df = pd.read_csv(os.path.join(input_dir, 'test.csv'))\n",
        "\n",
        "material_df = pd.read_csv(os.path.join(input_dir, 'materials.csv'))\n",
        "technique_df = pd.read_csv(os.path.join(input_dir, 'techniques.csv'))\n",
        "\n",
        "if IS_REGR:\n",
        "  train_df['label'] = train_df['target']\n",
        "  train_df['target'] = np.log1p(train_df['sorting_date'])\n",
        "  thresh_0 = np.log1p(1440)\n",
        "  thresh_1 = np.log1p(1600)\n",
        "  thresh_2 = np.log1p(1700)\n",
        "  thresh_3 = np.log1p(1800)\n",
        "  print(thresh_0, thresh_1, thresh_2, thresh_3)\n",
        "  train_df['label_log1p'] = train_df['label'].map({0: thresh_0, 1: thresh_1, 2: thresh_2, 3: thresh_3})\n",
        "  train_df['label_log1p'][:10]\n",
        "elif IS_SOFTLABEL:\n",
        "  train_df['label'] = train_df['target']\n",
        "  train_df['target'] = train_df['sorting_date'] / 100.0 - 15.51"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "XB-vVtlQqG6O",
        "outputId": "c66bfbfb-eab1-4926-f9a1-f16d8dcdcc0d"
      },
      "source": [
        "def to_img_path(object_id):\n",
        "    return os.path.join(photo_dir, f'{object_id}.jpg')\n",
        "\n",
        "def read_image(object_id):\n",
        "    return Image.open(to_img_path(object_id))\n",
        "\n",
        "img = read_image(train_df['object_id'].iat[0])\n",
        "img"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x109 at 0x7F3E0F667A50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSCTHz6TqJ0j"
      },
      "source": [
        "IMG_MEAN = [0.485, 0.456, 0.406]\n",
        "IMG_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "class AtmaDataset(data.Dataset):\n",
        "    \"\"\"atmaCup用にデータ読み込み等を行なうデータ・セット\"\"\"\n",
        "    object_path_key = \"object_path\"\n",
        "    label_key = \"target\"\n",
        "\n",
        "    @property\n",
        "    def meta_keys(self):\n",
        "        retval = [self.object_path_key]\n",
        "\n",
        "        if self.is_train:\n",
        "            retval += [self.label_key]\n",
        "\n",
        "        return retval\n",
        "\n",
        "    def __init__(self, meta_df: pd.DataFrame, is_train=True):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            meta_df: \n",
        "                画像へのパスと label 情報が含まれている dataframe\n",
        "                必ず object_path に画像へのパス, target に正解ラベルが入っている必要があります\n",
        "\n",
        "            is_train:\n",
        "                True のとき学習用のデータ拡張を適用します.\n",
        "                False の時は単に size にリサイズを行います\n",
        "        \"\"\"\n",
        "\n",
        "        self.is_train = is_train\n",
        "        for k in self.meta_keys:\n",
        "            if k not in meta_df:\n",
        "                raise ValueError(\"meta df must have {}\".format(k))\n",
        "\n",
        "        self.meta_df = meta_df.reset_index(drop=True)\n",
        "        self.index_to_data = self.meta_df.to_dict(orient=\"index\")\n",
        "\n",
        "        size = SIZE\n",
        "\n",
        "        additional_items = (\n",
        "            [T.Resize(size)]\n",
        "            if not is_train\n",
        "            else [\n",
        "                T.RandomGrayscale(p=0.2),\n",
        "                T.RandomVerticalFlip(),\n",
        "                T.RandomHorizontalFlip(),\n",
        "                T.ColorJitter(\n",
        "                    brightness=0.3,\n",
        "                    contrast=0.5,\n",
        "                    saturation=[0.8, 1.3],\n",
        "                    hue=[-0.05, 0.05],\n",
        "                ),\n",
        "                T.RandomResizedCrop(size),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.transformer = T.Compose(\n",
        "            [*additional_items, T.ToTensor(), T.Normalize(mean=IMG_MEAN, std=IMG_STD)]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.index_to_data[index]\n",
        "\n",
        "        obj_path, label = data.get(self.object_path_key), data.get(self.label_key, -1)\n",
        "        img = Image.open(obj_path)\n",
        "        img = self.transformer(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI_dEVkyqJyk"
      },
      "source": [
        "train_meta_df = train_df[['target', 'object_id']].copy()\n",
        "train_meta_df['object_path'] = train_meta_df['object_id'].map(to_img_path)\n",
        "\n",
        "dataset = AtmaDataset(meta_df=train_meta_df)\n",
        "loader = data.DataLoader(dataset=dataset, batch_size=54, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx1312gLqJvV",
        "outputId": "332fce6c-b3f6-471d-f82c-692a3304a2f0"
      },
      "source": [
        "assert torch.cuda.is_available()\n",
        "\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "print(DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV5x4mK8qJsy"
      },
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    optimizer: Optimizer,\n",
        "    train_loader: data.DataLoader\n",
        ") -> pd.Series:\n",
        "\n",
        "    # train にすることで model 内の学習時にのみ有効な機構が有効になります (Dropouts Layers、BatchNorm Layers...)\n",
        "    model.train()\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # ロスの値を保存する用に dict を用意\n",
        "    metrics = defaultdict(float)\n",
        "    n_iters = len(train_loader)\n",
        "\n",
        "    for i, (x_i, y_i) in enumerate(train_loader):\n",
        "        x_i = x_i.to(DEVICE)\n",
        "        y_i = y_i.to(DEVICE).reshape(-1, 1).float()\n",
        "\n",
        "        output = model(x_i)\n",
        "        loss = criterion(output, y_i)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_i = {\n",
        "            # loss は tensor object なので item をつかって python object に戻す\n",
        "            \"loss\": loss.item()\n",
        "        }\n",
        "        for k, v in metric_i.items():\n",
        "            metrics[k] += v\n",
        "\n",
        "    for k, v in metrics.items():\n",
        "        metrics[k] /= n_iters\n",
        "\n",
        "    return pd.Series(metrics).add_prefix(\"train_\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndoNi2WRqJpx"
      },
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "# fold = KFold(n_splits=5, shuffle=True, random_state=510)\n",
        "fold = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=510)\n",
        "if IS_REGR or IS_SOFTLABEL:\n",
        "  cv = list(fold.split(X=train_df, y=train_df['label'], groups=train_df['art_series_id']))\n",
        "else:\n",
        "  cv = list(fold.split(X=train_df, y=train_df['target'], groups=train_df['art_series_id']))\n",
        "  # cv = list(fold.split(X=train_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o1iftOSqT9q"
      },
      "source": [
        "import ttach as tta\n",
        "\n",
        "def predict(model: nn.Module, loader: data.DataLoader) -> np.ndarray:\n",
        "    # train とは逆で model 内の学習時にのみ有効な機構がオフになります (Dropouts Layers、BatchNorm Layers...)\n",
        "    model.eval()\n",
        "\n",
        "    predicts = []\n",
        "\n",
        "    # TTA\n",
        "    transforms = tta.Compose(\n",
        "        [\n",
        "            tta.HorizontalFlip(),\n",
        "            tta.VerticalFlip(),\n",
        "        ]\n",
        "    )\n",
        "    tta_model = tta.ClassificationTTAWrapper(model, transforms)\n",
        "\n",
        "    for x_i, y_i in loader:\n",
        "\n",
        "        # 明示的に勾配を計算しないように指定することができます. \n",
        "        # この関数ではモデルの更新はせずに単に出力だけを使いますので勾配は不要です.\n",
        "        with torch.no_grad():\n",
        "            output = tta_model(x_i.to(DEVICE))\n",
        "\n",
        "        predicts.extend(output.data.cpu().numpy())\n",
        "\n",
        "    pred = np.array(predicts).reshape(-1)\n",
        "    return pred\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred) -> dict:\n",
        "    \"\"\"正解ラベルと予測ラベルから指標を計算する\"\"\"\n",
        "    # return regression_metrics(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        'rmse': mean_squared_error(y_true, y_pred) ** .5\n",
        "    }\n",
        "\n",
        "\n",
        "def valid(\n",
        "    model: nn.Module, \n",
        "    y_valid: np.ndarray, \n",
        "    valid_loader: data.DataLoader\n",
        ") -> pd.Series:\n",
        "    \"\"\"検証フェーズ\n",
        "    与えられたモデル・データローダを使って検証フェーズを実行。スコアの dict と予測した値を返す\n",
        "    \"\"\"\n",
        "\n",
        "    pred = predict(model, valid_loader)\n",
        "    score = calculate_metrics(y_valid, pred)\n",
        "\n",
        "    valid_score = pd.Series(score)\n",
        "    return valid_score.add_prefix(\"valid_\"), pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRIukHDhqT8O"
      },
      "source": [
        "def run_fold(\n",
        "    model: nn.Module, \n",
        "    train_df: pd.DataFrame, \n",
        "    valid_df: pd.DataFrame, \n",
        "    y_valid: np.ndarray, \n",
        "    output_dir: str, \n",
        "    n_epochs) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    train / valid に分割されたデータで学習と同時に検証を行なう\n",
        "    \"\"\"\n",
        "    \n",
        "    os.makedirs(output_i, exist_ok=True)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=LERNING_RATE)\n",
        "    train_dataset = AtmaDataset(meta_df=train_df)\n",
        "    valid_dataset = AtmaDataset(meta_df=valid_df, is_train=False)\n",
        "\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
        "    valid_loader = data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
        "\n",
        "    score_df = pd.DataFrame()\n",
        "    valid_score = np.inf\n",
        "    valid_score_key = \"valid_rmse\"\n",
        "    valid_best_pred = None\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print(f'start epoch: {epoch}')\n",
        "\n",
        "        score_train = train(model, optimizer, train_loader)\n",
        "        score_valid, y_valid_pred = valid(model=model, valid_loader=valid_loader, y_valid=y_valid)\n",
        "\n",
        "\n",
        "        # --- 学習のロスと検証スコアの値をデータフレームに追加\n",
        "        row = pd.concat([score_train, score_valid])\n",
        "        row[\"epoch\"] = epoch\n",
        "        row = pd.DataFrame([row])\n",
        "        train_loss = row['train_loss'][0]\n",
        "        valid_rmse = row['valid_rmse'][0]\n",
        "        print(f'epoch: {epoch}, train_loss: {train_loss}, valid_rmse: {valid_rmse}')\n",
        "        # print(tabulate(row, headers=row.columns))\n",
        "        score_df = pd.concat([score_df, row], ignore_index=True)\n",
        "        # ---\n",
        "\n",
        "        # 今の検証スコアと過去最高のスコアを比較\n",
        "        current_score = score_valid[valid_score_key]\n",
        "        if current_score < valid_score:\n",
        "            # スコア改善したときモデルを保存する\n",
        "            print(f'validation score is improved!! {valid_score:.4f} -> {current_score:.4f}')\n",
        "            torch.save(\n",
        "                model.state_dict(), os.path.join(output_dir, 'model_best.pth')\n",
        "            )\n",
        "            valid_score = current_score\n",
        "            valid_best_pred = y_valid_pred\n",
        "\n",
        "    score_df.to_csv(os.path.join(output_dir, 'score.csv'), index=False)\n",
        "    return valid_best_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaFqTyQYqYBq"
      },
      "source": [
        "def make_simsiam_model(model_path):\n",
        "    # dimension of the embeddings\n",
        "    num_ftrs = 512\n",
        "    # dimension of the output of the prediction and projection heads\n",
        "    out_dim = proj_hidden_dim = 512\n",
        "    # the prediction head uses a bottleneck architecture\n",
        "    pred_hidden_dim = 128\n",
        "    # use 2 layers in the projection head\n",
        "    num_mlp_layers = 2\n",
        "\n",
        "    resnet = torchvision.models.resnet18(pretrained=False)\n",
        "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "    model = lightly.models.SimSiam(\n",
        "        backbone,\n",
        "        num_ftrs=num_ftrs,\n",
        "        proj_hidden_dim=pred_hidden_dim,\n",
        "        pred_hidden_dim=pred_hidden_dim,\n",
        "        out_dim=out_dim,\n",
        "        num_mlp_layers=num_mlp_layers\n",
        "    )\n",
        "    model = model.backbone\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    #最終層に全結合層を追加\n",
        "    model.add_module('flatten', nn.Flatten())\n",
        "    backbone.add_module('fc', nn.Linear(in_features=512, out_features=1, bias=True))\n",
        "    return backbone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUNY7S-FqT6k",
        "outputId": "10737622-36ea-4f03-cfc9-2b4586105358"
      },
      "source": [
        "%%time\n",
        "def get_output_dir(n_cv: int):\n",
        "    return os.path.join(output_dir, 'simsiam', f'cv={n_cv}')\n",
        "\n",
        "oof = np.zeros((len(train_df), ), dtype=np.float32)\n",
        "\n",
        "for i, (idx_tr, idx_valid) in enumerate(cv):\n",
        "    print(f'***** fold: {i} *****')\n",
        "    output_i = get_output_dir(i)\n",
        "    # model = resnet34(pretrained=False)\n",
        "    # model.fc = nn.Linear(in_features=512, out_features=1, bias=True)\n",
        "    model = make_simsiam_model(os.path.join(output_dir, './simsiam/model.pth'))\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    oof_i = run_fold(\n",
        "        model=model, \n",
        "        train_df=train_meta_df.iloc[idx_tr], \n",
        "        valid_df=train_meta_df.iloc[idx_valid], \n",
        "        y_valid=train_meta_df['target'].values[idx_valid],\n",
        "        output_dir=output_i,\n",
        "        n_epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    oof[idx_valid] = oof_i"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** fold: 0 *****\n",
            "start epoch: 1\n",
            "epoch: 1, train_loss: 1.6072179948797032, valid_rmse: 0.9601350407916712\n",
            "validation score is improved!! inf -> 0.9601\n",
            "start epoch: 2\n",
            "epoch: 2, train_loss: 0.9510107265443218, valid_rmse: 0.943698064399039\n",
            "validation score is improved!! 0.9601 -> 0.9437\n",
            "start epoch: 3\n",
            "epoch: 3, train_loss: 0.9416853402342115, valid_rmse: 0.9619919094621249\n",
            "start epoch: 4\n",
            "epoch: 4, train_loss: 0.924287740065127, valid_rmse: 0.9470976405362452\n",
            "start epoch: 5\n",
            "epoch: 5, train_loss: 0.9269635050880666, valid_rmse: 0.9172971450287365\n",
            "validation score is improved!! 0.9437 -> 0.9173\n",
            "start epoch: 6\n",
            "epoch: 6, train_loss: 0.926230250876777, valid_rmse: 0.9268039504507158\n",
            "start epoch: 7\n",
            "epoch: 7, train_loss: 0.9208164178595251, valid_rmse: 0.920053596781876\n",
            "start epoch: 8\n",
            "epoch: 8, train_loss: 0.9022926092147827, valid_rmse: 1.0487533344816382\n",
            "start epoch: 9\n",
            "epoch: 9, train_loss: 0.8965927113075646, valid_rmse: 0.9412927048158103\n",
            "start epoch: 10\n",
            "epoch: 10, train_loss: 0.8975176418922386, valid_rmse: 1.0147050644875548\n",
            "start epoch: 11\n",
            "epoch: 11, train_loss: 0.8877214394053634, valid_rmse: 1.013739243717892\n",
            "start epoch: 12\n",
            "epoch: 12, train_loss: 0.8977044154794849, valid_rmse: 1.0441434442381778\n",
            "start epoch: 13\n",
            "epoch: 13, train_loss: 0.8964140640229595, valid_rmse: 0.8807093580074906\n",
            "validation score is improved!! 0.9173 -> 0.8807\n",
            "start epoch: 14\n",
            "epoch: 14, train_loss: 0.8832195230892727, valid_rmse: 0.8817866482610568\n",
            "start epoch: 15\n",
            "epoch: 15, train_loss: 0.8862134467582313, valid_rmse: 0.9376007266891402\n",
            "start epoch: 16\n",
            "epoch: 16, train_loss: 0.8821590703969099, valid_rmse: 0.9361415958242937\n",
            "start epoch: 17\n",
            "epoch: 17, train_loss: 0.8807665325549184, valid_rmse: 0.8905735476569456\n",
            "start epoch: 18\n",
            "epoch: 18, train_loss: 0.8806042519150948, valid_rmse: 0.8711164959104953\n",
            "validation score is improved!! 0.8807 -> 0.8711\n",
            "start epoch: 19\n",
            "epoch: 19, train_loss: 0.8770691542601099, valid_rmse: 0.896797664801518\n",
            "start epoch: 20\n",
            "epoch: 20, train_loss: 0.8774261578005187, valid_rmse: 0.8790546894850462\n",
            "start epoch: 21\n",
            "epoch: 21, train_loss: 0.8730773682496986, valid_rmse: 1.1294935410003772\n",
            "start epoch: 22\n",
            "epoch: 22, train_loss: 0.8687192350625992, valid_rmse: 1.114700093744637\n",
            "start epoch: 23\n",
            "epoch: 23, train_loss: 0.8747888058423996, valid_rmse: 0.8771031830140938\n",
            "start epoch: 24\n",
            "epoch: 24, train_loss: 0.866396422592961, valid_rmse: 0.8746269493246349\n",
            "start epoch: 25\n",
            "epoch: 25, train_loss: 0.8817356542056921, valid_rmse: 0.8786127446276402\n",
            "start epoch: 26\n",
            "epoch: 26, train_loss: 0.8671644500323704, valid_rmse: 0.8856611438145509\n",
            "start epoch: 27\n",
            "epoch: 27, train_loss: 0.879884527654064, valid_rmse: 0.8830663372145802\n",
            "start epoch: 28\n",
            "epoch: 28, train_loss: 0.870776791353615, valid_rmse: 0.8884610241933543\n",
            "start epoch: 29\n",
            "epoch: 29, train_loss: 0.8499017120624075, valid_rmse: 0.8459283206268028\n",
            "validation score is improved!! 0.8711 -> 0.8459\n",
            "start epoch: 30\n",
            "epoch: 30, train_loss: 0.8612072899633524, valid_rmse: 0.9051441822555135\n",
            "***** fold: 1 *****\n",
            "start epoch: 1\n",
            "epoch: 1, train_loss: 1.6732384082005949, valid_rmse: 0.9700472612005159\n",
            "validation score is improved!! inf -> 0.9700\n",
            "start epoch: 2\n",
            "epoch: 2, train_loss: 0.9377008877238449, valid_rmse: 0.9463577306737881\n",
            "validation score is improved!! 0.9700 -> 0.9464\n",
            "start epoch: 3\n",
            "epoch: 3, train_loss: 0.9454257980901368, valid_rmse: 0.9800286544043344\n",
            "start epoch: 4\n",
            "epoch: 4, train_loss: 0.9459435298126571, valid_rmse: 0.9591820875525883\n",
            "start epoch: 5\n",
            "epoch: 5, train_loss: 0.9458230399355596, valid_rmse: 0.9564674998723101\n",
            "start epoch: 6\n",
            "epoch: 6, train_loss: 0.9352444209614579, valid_rmse: 0.9469816101235429\n",
            "start epoch: 7\n",
            "epoch: 7, train_loss: 0.9305643451457121, valid_rmse: 0.9629673352133971\n",
            "start epoch: 8\n",
            "epoch: 8, train_loss: 0.9126775720897986, valid_rmse: 1.0026514893785998\n",
            "start epoch: 9\n",
            "epoch: 9, train_loss: 0.8992012045821365, valid_rmse: 0.9445052688787184\n",
            "validation score is improved!! 0.9464 -> 0.9445\n",
            "start epoch: 10\n",
            "epoch: 10, train_loss: 0.9141773392959517, valid_rmse: 1.032167041441144\n",
            "start epoch: 11\n",
            "epoch: 11, train_loss: 0.9337257840195481, valid_rmse: 0.9132891541708785\n",
            "validation score is improved!! 0.9445 -> 0.9133\n",
            "start epoch: 12\n",
            "epoch: 12, train_loss: 0.915834440868728, valid_rmse: 0.9026809798405212\n",
            "validation score is improved!! 0.9133 -> 0.9027\n",
            "start epoch: 13\n",
            "epoch: 13, train_loss: 0.8956833092533812, valid_rmse: 0.999825338518987\n",
            "start epoch: 14\n",
            "epoch: 14, train_loss: 0.9075506971198686, valid_rmse: 0.913596320537819\n",
            "start epoch: 15\n",
            "epoch: 15, train_loss: 0.8936901205048269, valid_rmse: 0.9865849415404985\n",
            "start epoch: 16\n",
            "epoch: 16, train_loss: 0.8945800826257589, valid_rmse: 0.9430999603519548\n",
            "start epoch: 17\n",
            "epoch: 17, train_loss: 0.8963757564826887, valid_rmse: 0.9406058184201912\n",
            "start epoch: 18\n",
            "epoch: 18, train_loss: 0.8895517818781794, valid_rmse: 0.8987731378014723\n",
            "validation score is improved!! 0.9027 -> 0.8988\n",
            "start epoch: 19\n",
            "epoch: 19, train_loss: 0.8865462915629757, valid_rmse: 1.0212392833652206\n",
            "start epoch: 20\n",
            "epoch: 20, train_loss: 0.8905964305206221, valid_rmse: 0.905954118056959\n",
            "start epoch: 21\n",
            "epoch: 21, train_loss: 0.8729390678357105, valid_rmse: 0.9242261663833757\n",
            "start epoch: 22\n",
            "epoch: 22, train_loss: 0.8905223796562273, valid_rmse: 1.0163287568974482\n",
            "start epoch: 23\n",
            "epoch: 23, train_loss: 0.8780885670258074, valid_rmse: 0.9292110971005766\n",
            "start epoch: 24\n",
            "epoch: 24, train_loss: 0.8781982362270355, valid_rmse: 0.9017900560367681\n",
            "start epoch: 25\n",
            "epoch: 25, train_loss: 0.8788720746429599, valid_rmse: 1.0422135509299777\n",
            "start epoch: 26\n",
            "epoch: 26, train_loss: 0.8671534906844703, valid_rmse: 0.872254205396339\n",
            "validation score is improved!! 0.8988 -> 0.8723\n",
            "start epoch: 27\n",
            "epoch: 27, train_loss: 0.869871700600702, valid_rmse: 0.8935781758916466\n",
            "start epoch: 28\n",
            "epoch: 28, train_loss: 0.8634451326667046, valid_rmse: 0.9476636911457388\n",
            "start epoch: 29\n",
            "epoch: 29, train_loss: 0.8818756536561616, valid_rmse: 0.8827316459883482\n",
            "start epoch: 30\n",
            "epoch: 30, train_loss: 0.8682640079332857, valid_rmse: 1.0183564783678958\n",
            "***** fold: 2 *****\n",
            "start epoch: 1\n",
            "epoch: 1, train_loss: 1.5938950235746345, valid_rmse: 0.9679488209836741\n",
            "validation score is improved!! inf -> 0.9679\n",
            "start epoch: 2\n",
            "epoch: 2, train_loss: 0.9374114807162967, valid_rmse: 0.9807450877524841\n",
            "start epoch: 3\n",
            "epoch: 3, train_loss: 0.9329643693505502, valid_rmse: 0.9648891906242605\n",
            "validation score is improved!! 0.9679 -> 0.9649\n",
            "start epoch: 4\n",
            "epoch: 4, train_loss: 0.9157842695713043, valid_rmse: 0.9841428940386705\n",
            "start epoch: 5\n",
            "epoch: 5, train_loss: 0.9024790619709053, valid_rmse: 1.027302513989224\n",
            "start epoch: 6\n",
            "epoch: 6, train_loss: 0.9108821853083007, valid_rmse: 1.1181078933668236\n",
            "start epoch: 7\n",
            "epoch: 7, train_loss: 0.8893985559745711, valid_rmse: 0.9645781563527481\n",
            "validation score is improved!! 0.9649 -> 0.9646\n",
            "start epoch: 8\n",
            "epoch: 8, train_loss: 0.891804247486348, valid_rmse: 0.9599997565103761\n",
            "validation score is improved!! 0.9646 -> 0.9600\n",
            "start epoch: 9\n",
            "epoch: 9, train_loss: 0.8932152968280169, valid_rmse: 0.9959110656555887\n",
            "start epoch: 10\n",
            "epoch: 10, train_loss: 0.884279308270435, valid_rmse: 0.9314213313076866\n",
            "validation score is improved!! 0.9600 -> 0.9314\n",
            "start epoch: 11\n",
            "epoch: 11, train_loss: 0.8824389370120301, valid_rmse: 0.9193838975669865\n",
            "validation score is improved!! 0.9314 -> 0.9194\n",
            "start epoch: 12\n",
            "epoch: 12, train_loss: 0.8766954924379077, valid_rmse: 0.951401746347486\n",
            "start epoch: 13\n",
            "epoch: 13, train_loss: 0.8830776080793264, valid_rmse: 0.9252848685165226\n",
            "start epoch: 14\n",
            "epoch: 14, train_loss: 0.8590658145899676, valid_rmse: 0.911988931930935\n",
            "validation score is improved!! 0.9194 -> 0.9120\n",
            "start epoch: 15\n",
            "epoch: 15, train_loss: 0.8570365522589002, valid_rmse: 0.9261450425753245\n",
            "start epoch: 16\n",
            "epoch: 16, train_loss: 0.8766189588575947, valid_rmse: 0.9373285080469143\n",
            "start epoch: 17\n",
            "epoch: 17, train_loss: 0.8684262639405776, valid_rmse: 0.9311017746720601\n",
            "start epoch: 18\n",
            "epoch: 18, train_loss: 0.8630240228103132, valid_rmse: 0.912667357083054\n",
            "start epoch: 19\n",
            "epoch: 19, train_loss: 0.86745158993468, valid_rmse: 0.9383334957953827\n",
            "start epoch: 20\n",
            "epoch: 20, train_loss: 0.8588861871738823, valid_rmse: 0.9243954228067539\n",
            "start epoch: 21\n",
            "epoch: 21, train_loss: 0.8727509446290075, valid_rmse: 0.9182550469815045\n",
            "start epoch: 22\n",
            "epoch: 22, train_loss: 0.8490301990995601, valid_rmse: 0.9525786277158548\n",
            "start epoch: 23\n",
            "epoch: 23, train_loss: 0.8430126923687604, valid_rmse: 1.1299514239131099\n",
            "start epoch: 24\n",
            "epoch: 24, train_loss: 0.8514346282701103, valid_rmse: 0.9333028501575258\n",
            "start epoch: 25\n",
            "epoch: 25, train_loss: 0.8619607011882626, valid_rmse: 1.0714874685958995\n",
            "start epoch: 26\n",
            "epoch: 26, train_loss: 0.8386142153520973, valid_rmse: 0.9402426487483295\n",
            "start epoch: 27\n",
            "epoch: 27, train_loss: 0.8468146473169327, valid_rmse: 0.9594808061926311\n",
            "start epoch: 28\n",
            "epoch: 28, train_loss: 0.8384653570092454, valid_rmse: 0.9714708289585272\n",
            "start epoch: 29\n",
            "epoch: 29, train_loss: 0.8524111652252625, valid_rmse: 0.899122918239155\n",
            "validation score is improved!! 0.9120 -> 0.8991\n",
            "start epoch: 30\n",
            "epoch: 30, train_loss: 0.8348765038714117, valid_rmse: 0.927011469517553\n",
            "***** fold: 3 *****\n",
            "start epoch: 1\n",
            "epoch: 1, train_loss: 1.6353658753998426, valid_rmse: 0.9909363465853905\n",
            "validation score is improved!! inf -> 0.9909\n",
            "start epoch: 2\n",
            "epoch: 2, train_loss: 0.9314863487165801, valid_rmse: 0.9926886532883223\n",
            "start epoch: 3\n",
            "epoch: 3, train_loss: 0.9277672512190682, valid_rmse: 0.9834228050054179\n",
            "validation score is improved!! 0.9909 -> 0.9834\n",
            "start epoch: 4\n",
            "epoch: 4, train_loss: 0.9115243432473163, valid_rmse: 0.9867432234915378\n",
            "start epoch: 5\n",
            "epoch: 5, train_loss: 0.9201818032532322, valid_rmse: 1.004956178312319\n",
            "start epoch: 6\n",
            "epoch: 6, train_loss: 0.9090317238350304, valid_rmse: 0.9610268962037116\n",
            "validation score is improved!! 0.9834 -> 0.9610\n",
            "start epoch: 7\n",
            "epoch: 7, train_loss: 0.9103494900829938, valid_rmse: 0.9712495767369973\n",
            "start epoch: 8\n",
            "epoch: 8, train_loss: 0.9059032706581817, valid_rmse: 0.9925839929309299\n",
            "start epoch: 9\n",
            "epoch: 9, train_loss: 0.8950939853580631, valid_rmse: 0.9740108973557401\n",
            "start epoch: 10\n",
            "epoch: 10, train_loss: 0.8835362910616155, valid_rmse: 0.9602371143180063\n",
            "validation score is improved!! 0.9610 -> 0.9602\n",
            "start epoch: 11\n",
            "epoch: 11, train_loss: 0.8779765650325891, valid_rmse: 0.9639941486528316\n",
            "start epoch: 12\n",
            "epoch: 12, train_loss: 0.8743091140474591, valid_rmse: 0.9639943481643175\n",
            "start epoch: 13\n",
            "epoch: 13, train_loss: 0.8665993621154707, valid_rmse: 0.9536227227136499\n",
            "validation score is improved!! 0.9602 -> 0.9536\n",
            "start epoch: 14\n",
            "epoch: 14, train_loss: 0.8609748008299847, valid_rmse: 0.9345110299531234\n",
            "validation score is improved!! 0.9536 -> 0.9345\n",
            "start epoch: 15\n",
            "epoch: 15, train_loss: 0.8572516937036904, valid_rmse: 0.9477430106885006\n",
            "start epoch: 16\n",
            "epoch: 16, train_loss: 0.8730724657676658, valid_rmse: 1.1117948661809938\n",
            "start epoch: 17\n",
            "epoch: 17, train_loss: 0.8507169223561579, valid_rmse: 0.9247971271674746\n",
            "validation score is improved!! 0.9345 -> 0.9248\n",
            "start epoch: 18\n",
            "epoch: 18, train_loss: 0.866045164514561, valid_rmse: 0.928689491849142\n",
            "start epoch: 19\n",
            "epoch: 19, train_loss: 0.8545277112600754, valid_rmse: 0.9967140447614661\n",
            "start epoch: 20\n",
            "epoch: 20, train_loss: 0.8576537096986965, valid_rmse: 0.9433574614308428\n",
            "start epoch: 21\n",
            "epoch: 21, train_loss: 0.8595509669002221, valid_rmse: 1.141317362123459\n",
            "start epoch: 22\n",
            "epoch: 22, train_loss: 0.8576019658726088, valid_rmse: 0.9543048059223185\n",
            "start epoch: 23\n",
            "epoch: 23, train_loss: 0.8516046313606963, valid_rmse: 0.9367370099440093\n",
            "start epoch: 24\n",
            "epoch: 24, train_loss: 0.8509981273388376, valid_rmse: 1.005353102483676\n",
            "start epoch: 25\n",
            "epoch: 25, train_loss: 0.8435822895595005, valid_rmse: 0.9274047805366405\n",
            "start epoch: 26\n",
            "epoch: 26, train_loss: 0.8509853652545384, valid_rmse: 1.0477271064214724\n",
            "start epoch: 27\n",
            "epoch: 27, train_loss: 0.8513483578453258, valid_rmse: 0.9586411561420711\n",
            "start epoch: 28\n",
            "epoch: 28, train_loss: 0.8500486989410556, valid_rmse: 0.9156216349129751\n",
            "validation score is improved!! 0.9248 -> 0.9156\n",
            "start epoch: 29\n",
            "epoch: 29, train_loss: 0.8404938399183507, valid_rmse: 0.9387627366565894\n",
            "start epoch: 30\n",
            "epoch: 30, train_loss: 0.8574629638876233, valid_rmse: 1.0045014946875952\n",
            "***** fold: 4 *****\n",
            "start epoch: 1\n",
            "epoch: 1, train_loss: 1.692275729106397, valid_rmse: 1.0433228869615971\n",
            "validation score is improved!! inf -> 1.0433\n",
            "start epoch: 2\n",
            "epoch: 2, train_loss: 0.9472693557641945, valid_rmse: 0.9625550766511041\n",
            "validation score is improved!! 1.0433 -> 0.9626\n",
            "start epoch: 3\n",
            "epoch: 3, train_loss: 0.9493439392167695, valid_rmse: 0.9562874922437012\n",
            "validation score is improved!! 0.9626 -> 0.9563\n",
            "start epoch: 4\n",
            "epoch: 4, train_loss: 0.936202271860473, valid_rmse: 0.9540013184382271\n",
            "validation score is improved!! 0.9563 -> 0.9540\n",
            "start epoch: 5\n",
            "epoch: 5, train_loss: 0.9329803208915555, valid_rmse: 0.9539892864273055\n",
            "validation score is improved!! 0.9540 -> 0.9540\n",
            "start epoch: 6\n",
            "epoch: 6, train_loss: 0.9355532368835138, valid_rmse: 0.9528548246038037\n",
            "validation score is improved!! 0.9540 -> 0.9529\n",
            "start epoch: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1A_IpxXqT45"
      },
      "source": [
        "sns.distplot(oof)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRB6eDBVqT2h"
      },
      "source": [
        "if IS_SOFTLABEL:\n",
        "  oof = np.clip(oof, 0, 3)\n",
        "print(calculate_metrics(train_df['target'], oof))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NUGs_FaqTzU"
      },
      "source": [
        "def create_metadata(input_df):\n",
        "    out_df = input_df[['object_id']].copy()\n",
        "    out_df['object_path'] = input_df['object_id'].map(to_img_path)\n",
        "\n",
        "    if \"target\" in input_df:\n",
        "        out_df[\"target\"] = input_df[\"target\"]\n",
        "\n",
        "    return out_df\n",
        "\n",
        "# train と似たようなことをするので、次回から楽したいとおもって `create_metadata` という関数を作りました\n",
        "test_meta_df = create_metadata(test_df)\n",
        "\n",
        "# 学習時のデータ拡張はオフにしたいので is_train=False としている\n",
        "test_dataset = AtmaDataset(meta_df=test_meta_df, is_train=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=128, drop_last=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiMc0p6WqTuB"
      },
      "source": [
        "%%time\n",
        "test_predictions = []\n",
        "\n",
        "for i in range(len(cv)):\n",
        "    print(f'cv: {i}')\n",
        "    output_i = get_output_dir(i)\n",
        "\n",
        "    # model = resnet34(pretrained=False)\n",
        "    # model = resnet18(pretrained=False)\n",
        "    # model.fc = nn.Linear(in_features=512, out_features=1, bias=True)\n",
        "    # model_path = os.path.join(output_i, 'model_best.pth')\n",
        "    model = make_simsiam_model(torch.load(model_path))\n",
        "\n",
        "    # model.load_state_dict(torch.load(model_path))\n",
        "    model.to(DEVICE)\n",
        "    y_pred_i = predict(model, loader=test_loader)\n",
        "    test_predictions.append(y_pred_i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I57y8JXvr1-1"
      },
      "source": [
        "pred_mean = np.array(test_predictions).mean(axis=0)\n",
        "if IS_SOFTLABEL:\n",
        "  pred_mean = np.clip(pred_mean, 0, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKCYg-P7r179"
      },
      "source": [
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "print(now)\n",
        "# sub.to_csv(f'../output/submission_{now.strftime(\"%m%d_%H%M\")}.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGnyAZior15Y"
      },
      "source": [
        "pred_mean = np.array(test_predictions).mean(axis=0)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"target\": pred_mean\n",
        "}).to_csv(os.path.join(output_dir, f\"submission_{now.strftime('%m%d_%H%M')}.csv\"), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiLb_qcqr13K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1sfufggr1wB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}